##############################################################################
# The MIT License (MIT)
#
# Copyright (c) 2022-2024 NVIDIA CORPORATION
#
#Permission is hereby granted, free of charge, to any person obtaining a copy of
#this software and associated documentation files (the "Software"), to deal in
#the Software without restriction, including without limitation the rights to
#use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of
#the Software, and to permit persons to whom the Software is furnished to do so,
#subject to the following conditions:
#
#The above copyright notice and this permission notice shall be included in all
#copies or substantial portions of the Software.
#
#THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
#IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
#FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
#COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
#IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
#CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
################################################################################
This sample builds on top of the "faciallandmarks" graph to demonstrate how to
use NvDsEmotionExt related extensions.

The graph contains additional "NvDsVideoTemplate" and "NvDsEmotionTemplateLib"
components required to identify human emotions.

================================================================================
Pre-requisites
================================================================================
- DeepStreamSDK 7.0
- Graph-Composer 4.0.0
- Sync extensions from NGC public repo if not already done:
    $ registry repo sync -n ngc-public

================================================================================
Download models for emotion
================================================================================
1. Prepare facenet, faciallandmarks models according to faciallandmarks sample graph README
2. sudo mkdir -p /opt/nvidia/deepstream/deepstream/samples/models/tao_pretrained_models/emotion/
3. sudo wget https://api.ngc.nvidia.com/v2/models/nvidia/tao/emotionnet/versions/deployable_v1.0/files/model.etlt \
    -O /opt/nvidia/deepstream/deepstream/samples/models/tao_pretrained_models/emotion/emotion.etlt

Refer https://github.com/NVIDIA-AI-IOT/deepstream_tao_apps/tree/release/tao3.0/apps/tao_others#readme
for more info

================================================================================
Running the graph
================================================================================
$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/nvidia/deepstream/deepstream/lib/cvcore_libs/
$ chmod 775 /opt/nvidia/deepstream/deepstream/samples/models/tao_pretrained_models/faciallandmarks/
$ chmod 775 /opt/nvidia/deepstream/deepstream/samples/models/tao_pretrained_models/facenet
$ chmod 775 /opt/nvidia/deepstream/deepstream/samples/models/tao_pretrained_models/emotion
For x86_64 platform
$ /opt/nvidia/graph-composer/execute_graph.sh App-Emotion.yaml App-Emotion.parameters.yaml -d /opt/nvidia/deepstream/deepstream/reference_graphs/common/target_x86_64.yaml
For Jetson
$ /opt/nvidia/graph-composer/execute_graph.sh App-Emotion.yaml App-Emotion.parameters.yaml -d /opt/nvidia/deepstream/deepstream/reference_graphs/common/target_aarch64.yaml

NOTE: App-Emotion.yaml is the main graph file describing the DeepStream
graph(pipeline) along with the configuration parameters for the components in the
graph. App-Emotion.parameters.yaml can be used to override these parameter values.
